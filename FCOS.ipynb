{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of the Fully Connected One Stage (FCOS) object detection algorithm with training/eval on the PASCAL VOC 2007 dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the data has been downloaded (http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar) and extracted. The 'JPEGImages' subdirectory contains all the raw jpeg images. The 'Annotations' subdirectory contains corresponding XML files with object detection labels/metadata. The 'ImageSets/Main' subdirectory contains .txt files 'train.txt', 'val.txt' which contain identifiers of images for training and validation splits respectively. (There are also additional .txt files containing identifiers for images per class for each split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num train images: 2501\n",
      "Num val images: 2510\n"
     ]
    }
   ],
   "source": [
    "# first lets read in the image identifiers for train-val splits\n",
    "with open(os.path.join('VOC2007_trainval', 'ImageSets', 'Main', 'train.txt')) as file:\n",
    "    identifiers_train = [line.strip() for line in file.readlines()]\n",
    "\n",
    "with open(os.path.join('VOC2007_trainval', 'ImageSets', 'Main', 'val.txt')) as file:\n",
    "    identifiers_val = [line.strip() for line in file.readlines()]\n",
    "\n",
    "# now get the jpeg filepaths for the images\n",
    "image_filepaths_train = [os.path.join('VOC2007_trainval','JPEGImages',x+'.jpg') for x in identifiers_train]    \n",
    "image_filepaths_val = [os.path.join('VOC2007_trainval','JPEGImages',x+'.jpg') for x in identifiers_val]    \n",
    "\n",
    "# get the xml filepaths to object detection target labels\n",
    "target_filepaths_train = [os.path.join('VOC2007_trainval','Annotations',x+'.xml') for x in identifiers_train]    \n",
    "target_filepaths_val = [os.path.join('VOC2007_trainval','Annotations',x+'.xml') for x in identifiers_val]    \n",
    "\n",
    "print(f\"Num train images: {len(image_filepaths_train)}\")\n",
    "print(f\"Num val images: {len(image_filepaths_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set up a pytorch Dataset object for accessing image-target pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOC2007(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_dir='VOC2007_trainval', split='train', image_size=224, num_boxes=40):\n",
    "        super().__init__()\n",
    "        self.image_size = image_size\n",
    "        self.num_boxes = num_boxes # max number of object boxes per image\n",
    "\n",
    "        # first lets read in the image identifiers for train-val splits\n",
    "        with open(os.path.join(dataset_dir, 'ImageSets', 'Main', split+'.txt')) as file:\n",
    "            identifiers = [line.strip() for line in file.readlines()]\n",
    "        # now get the jpeg filepaths for the images\n",
    "        self.image_filepaths = [os.path.join(dataset_dir,'JPEGImages',x+'.jpg') for x in identifiers]    \n",
    "        # get the xml filepaths to object detection target labels\n",
    "        self.target_filepaths = [os.path.join(dataset_dir,'Annotations',x+'.xml') for x in identifiers]    \n",
    "        # get the target label annotations for all images\n",
    "        self.image_annotations = [self.parse_xml(filepath) for filepath in self.target_filepaths]\n",
    "        # class names\n",
    "        voc_classes = [\"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\", \"dog\",\n",
    "            \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]\n",
    "        # class name <-> integer mapping\n",
    "        self.class2idx = {c:i for i,c in enumerate(voc_classes)}\n",
    "        self.idx2class = {i:c for i,c in enumerate(voc_classes)}\n",
    "        \n",
    "        # define transformations for image preprocessing\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.Resize(self.image_size), # resize maintining original aspect ratio\n",
    "            transforms.CenterCrop(self.image_size), # do center crop to make square image\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # normalzie to have same mean and std as ImageNet\n",
    "        ])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filepaths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # get image filepath and annotations dictionary\n",
    "        image_filepath, annotation = self.image_filepaths[index], self.image_annotations[index] \n",
    "        # load the RGB image\n",
    "        image = Image.open(image_filepath).convert('RGB')\n",
    "        # make tensor of class labels per box\n",
    "        gt_classes = torch.tensor([self.class2idx[obj['class']] for obj in annotation['objects']]).unsqueeze(1) # shape: (N,1)\n",
    "        # make tensor of boxes (N boxes, each with xyxy boundiong box coordinates)\n",
    "        gt_boxes = torch.tensor([obj['bndbox'] for obj in annotation['objects']], dtype=torch.float) # shape: (N,4)\n",
    "        # normalize bounding box coordinates to be in [0,1] w.r.t. original image size\n",
    "        W, H = image.size\n",
    "        norm = torch.tensor([[W,H,W,H]])\n",
    "        gt_boxes /= norm \n",
    "        # preprocess image, transform to tensor\n",
    "        image = self.transforms(image)    \n",
    "       \n",
    "        # now transform the bounding boxes accordingly to match the transformed image dimensions\n",
    "        if H >= W:\n",
    "            # size of transforms.Resize() \n",
    "            # (if H>=W, then transforms.Resize() output size will be (H*image_size/W, image_size) else output size will be (image_size,W*image_size/H))\n",
    "            H_new = H * self.image_size / W\n",
    "            W_new = self.image_size \n",
    "        else:\n",
    "            H_new = self.image_size\n",
    "            W_new = W * self.image_size / H\n",
    "        # center-crop shift amount        \n",
    "        dxc = (W_new - self.image_size) // 2    \n",
    "        dyc = (H_new - self.image_size) // 2    \n",
    "\n",
    "        # unnormalize bounding box coordinates and shift for center-crop, also clamp to (0,image_size)\n",
    "        gt_boxes[:,0] = torch.clamp(gt_boxes[:,0] * W_new - dxc ,min=0)\n",
    "        gt_boxes[:,1] = torch.clamp(gt_boxes[:,1] * H_new - dyc ,min=0)\n",
    "        gt_boxes[:,2] = torch.clamp(gt_boxes[:,2] * W_new - dxc ,max=self.image_size)\n",
    "        gt_boxes[:,3] = torch.clamp(gt_boxes[:,3] * H_new - dyc ,max=self.image_size)\n",
    "\n",
    "        # concatenate bounding box coordinates and class labels into single tensor of shape (N,5)\n",
    "        gt_boxes = torch.cat([gt_boxes, gt_classes], dim=1)\n",
    "        # set invalid boxes to -1\n",
    "        invalid = (gt_boxes[:,0] > gt_boxes[:,2]) | (gt_boxes[:,1] > gt_boxes[:,3]) \n",
    "        gt_boxes[invalid] = -1\n",
    "        # apply padding to make N fixed size\n",
    "        gt_boxes = torch.cat([gt_boxes[:self.num_boxes], torch.full((self.num_boxes-len(gt_boxes),5), -1.0)])\n",
    "\n",
    "        return image_filepath, image, gt_boxes\n",
    "\n",
    "    # fucntion for parsing XML file to get object detection target labels\n",
    "    def parse_xml(self, filepath):\n",
    "        # start at the root of the XML tree\n",
    "        root_node = ET.parse(filepath).getroot()\n",
    "        annotations = {}\n",
    "        # get all the object bounding boxes\n",
    "        objects = []\n",
    "        for obj in root_node.findall('object'):\n",
    "            # for each object, get class name, difficulty identifier (0: easy, 1: difficult) and bounding box (xyxy: top-left and bottom-right corner coordinates) \n",
    "            object_dict = {\n",
    "                'class': obj.find('name').text,\n",
    "                'difficult': obj.find('difficult').text,\n",
    "                'bndbox': [int(obj.find('bndbox/xmin').text), \n",
    "                           int(obj.find('bndbox/xmin').text),\n",
    "                           int(obj.find('bndbox/xmin').text), \n",
    "                           int(obj.find('bndbox/xmin').text)]\n",
    "\n",
    "            }\n",
    "            objects.append(object_dict)\n",
    "        annotations['objects'] = objects\n",
    "        return annotations    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train-val datasets\n",
    "train_dataset = VOC2007(split='train')\n",
    "val_dataset = VOC2007(split='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
